#Assignment 1 


## 1.1 



```{r }
## 1.1 

cvLM <- function(Y, X, Nfolds){
  library(ggplot2)
  # DEFINING THE FUNCTION LINREG, FOR FITTING LINEAR MODELS 
  
  linreg<-function(formula,data){
    formula <- formula(formula)
    des.mat <- model.matrix(formula , data) #Extracts the model matrix
    dep.var <- all.vars(formula)[1]         #Extracts the name of the y-variable 
    dep.var <- as.matrix(data[dep.var])     #Extracts the data of the y-variable 
                                            #and overwrites it with the data-colum 
    
    #Calculating the beta coeffs. (X' %*% X)^-1 %*% X' %*% y
    beta.hat <- solve( t(des.mat) %*% des.mat )  %*% t(des.mat) %*% dep.var
    
    # Calculating the y-hat  , y_hat = X %*% beta_hat
    y.hat <- des.mat %*% beta.hat  
    
    #Calculating the residuals e= y- y_hat
    res.err <- dep.var - y.hat   
    
    l<-list( beta.hat = beta.hat, y.hat = y.hat, res.err = res.err)
    return(l)
  }
  
  
  #GENERATING ALL POSSIBLE PERMUTATIONS OF MODELS
  
  #Get the colnames for the X-variables
  q<-rep(paste0("X",c(1:5)))
  
  #Merge the data in to one data set and naming the columns
  myData <- cbind(Y,X) 
  colnames(myData)<- c("Y",q)
  
  #Generating all possible combinations 
  myComb<-sapply(c(1:5), FUN = combn, x = q )
  #Creating the vector that will hold all formulas 
  myformula <- c(myComb[[1]])
 
  #Extracting the combinations of 2 and 3 X-variables and adding a + between them
  for (i in 2:3){
    for (j in 1:10){
      myformula[length(myformula)+1]<-(paste(myComb[[i]][,j],collapse = " + "))
    }
  }
  
  #Heres two rows that could replace the above for-loop
  #sapply(2:3, FUN = function(i) sapply(1:10, FUN = function(j)
  #paste(myComb[[i]][,j], collapse = " + " ) ) )
  
  
  #Extracting the combinations of 4 and 5 X-variables and adding a + between them
  #This is basicly a loop for the 4 combinations 
  myformula <-c(myformula ,
                sapply(1:5, FUN =function(X)
                  paste(myComb[[4]][,X],collapse = " + " ) 
                  ),  paste(myComb[[5]],collapse = " + ")  
                )
  
 myformula<- paste("Y","~",myformula)
 
 #### SPLITTING AND SUBSETING DATA IN TO K FOLDS
 
 #calculatin no. rows
 noobs<-dim(myData)[1]
 K <- Nfolds
 
 #Use sample to randomly draw the indexes of the dataset
 #and reorder the data with them in a random manner
 set.seed(12345)
 myData<-myData[sample(noobs),] 
 
 #Create K equal indexes that are added to the data.
 cut(1:noobs,breaks=K,labels = FALSE)
 
 myData$index <- cut(1:noobs,breaks=K,labels = FALSE)
 

 #init a counting vector "o" useed to loop in to the data.frame "linearModels"
 #and a combination index "dataKombs" used for subseting the different datasets
 #used for fitting models
 o <- 1 
 linearModels<-data.frame(CV=1,model="text",nofeats=1,stringsAsFactors = FALSE)
 dataKombs <- combn(1:K,K-1)
 for (m in 1:length(myformula)){   
 for (l in (1:K)){  

   #the data of the K-folds used for the model estimation    
   data<-subset(myData, myData$index %in% dataKombs[,l] )


   #the fold that was left out in the model estimation
   predmatrix<-model.matrix(formula(myformula[m]), 
             subset(myData, !(myData$index %in% dataKombs[,l] ))) 
   
   

   #Calculating the CV score for each model. sum((Y - Y(hat))^2)
   CV<-sum(
     (  
       #this is the observed Y for the left out fold.
       subset(myData, !(myData$index %in% dataKombs[,l] ))[,1] -
        #predmatrix description above.
        predmatrix %*%  
        #the estimated beta-hats. 
        linreg(formula = myformula[m], data = data)$beta.hat 
       )^2
     )
   
   #inserting the results in to the linearModels data.frame
   linearModels[o,] <- c(CV,myformula[m],ncol(predmatrix) - 1)
   o <- o + 1 
    }
 }
 #reforming data to numeric again. 
 linearModels[,1] <- as.numeric( linearModels[,1])
 linearModels[,3] <- as.numeric( linearModels[,3])
   
 #The mean for the different models, each model is estimadet K times  
 plotdata<-suppressWarnings(aggregate(linearModels,by = list(linearModels$model),FUN= mean)[,-3])
 
 #renaming a column to ease plotting
 colnames(plotdata)[1] <- "Model"
 
 #plotting
 engr<-ggplot() +
   geom_line(data = aggregate(plotdata,list(plotdata$nofeats),FUN = min)[,c(3,4)],aes(x=nofeats,y=CV)) +
   geom_point(data = plotdata,aes(x=nofeats,y=CV,col = factor(nofeats)) ) +
       labs(title = "CV scores for different no. feats",color = "No. feat")
 
 
#displays the plot 
plot(engr)

 #Returns the models with the lowest average CV-score 
 return( plotdata[min(plotdata$CV) == plotdata$CV,c(1,3,2)])

}  

```

##1.2 

```{r swiss, echo=TRUE}
##1.2 

cvLM(Y = swiss[,1], 
     X = swiss[ , 2:ncol(swiss) ], 
     Nfolds = 5 )
```

For the plot above we conclude that the model with all five features, one of the models with four features and one of the models with three features are the best options, and they are almost equally good. If you look closely, you can see that the lowest CV-score seems to belong to one of the four features models.
The models with ony one explanatory variable are worse models than the other, which is valid for the models with two features as well. The plot shows how both the variation and the mean of the CV score decreases when the number of features increases.

The best model is a model with four features, with the features X1, X3, X4 and X5.
That is the variables \textit{Agriculture, Education, Catholic} and \textit{Infant Mortality}. The variable \textit{Fertility} is the response variable. We think it is a reasonable result, even though we have a hard time describing why the share of males involved in the agriculture should impact the fertility.  But probably in general the chosen variables serves as good indicators for well-being in the towns which could impact the fertility rate. The share of Catholics in the town could be explained by their belief that various methods of birth-control is some kind of a sin. 

## Assignment 2 

## 2.1

A linear model with Protein as a predictor would be a good fit to the variable Moisture. There are some outliers that could affect the fit in a negative way but in general we think that a linear model fit the data well. 


## 2.2

Jupp 


## 2.3 

The plot above indicates that the linear model without higher degree of polynomials is the best. The model with a first degree polynomial has the lowest MSE for the test set. The MSE for all other powers of the polynom is worse for the test data. Each of the added polynomial terms increases the overfitting of the model. The MSE for the training dataset decreases as we add more terms, which is reasonable. 


